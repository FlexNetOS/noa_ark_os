#!/usr/bin/env python3
"""Ensure the local pipeline executed for the current commit before remote workflows continue."""

from __future__ import annotations

import argparse
import datetime as dt
import hashlib
import json
import os
import pathlib
import subprocess
import sys
from typing import Any, Dict


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Validate local pipeline evidence")
    parser.add_argument(
        "--status-file",
        default="audit/local_pipeline_status.json",
        help="Path to the JSON file generated by scripts/pipeline/record_local_pipeline.sh",
    )
    parser.add_argument(
        "--max-age-minutes",
        type=int,
        default=int(os.environ.get("LOCAL_PIPELINE_MAX_AGE_MINUTES", "360")),
        help="Maximum allowed age for the evidence. Use 0 or a negative value to skip the age check.",
    )
    parser.add_argument(
        "--allow-empty",
        action="store_true",
        help="Allow an uninitialized status file without failing (useful for local dry-runs)",
    )
    parser.add_argument(
        "--require-logs",
        action="store_true",
        help="Fail if the referenced build/test logs are missing or do not match their recorded hashes.",
    )
        parser.add_argument(
            "--skip-if-missing",
            action="store_true",
            help="Do not fail when the status file is absent (useful for fresh clones or CI).",
        )
        parser.add_argument(
            "--ci-optional",
            action="store_true",
            help="Downgrade failures to warnings when running inside GitHub Actions.",
        )
    return parser.parse_args()


def fail(message: str) -> "None":
    print(f"❌ {message}", file=sys.stderr)
    sys.exit(1)


def sha256(path: pathlib.Path) -> str:
    digest = hashlib.sha256()
    with path.open("rb") as handle:
        for chunk in iter(lambda: handle.read(1024 * 1024), b""):
            digest.update(chunk)
    return digest.hexdigest()


def as_datetime(raw: str) -> dt.datetime:
    # Support timestamps with or without trailing Z
    normalized = raw.rstrip("Z")
    if normalized.endswith("+00:00"):
        normalized = normalized.replace("+00:00", "")
    return dt.datetime.fromisoformat(normalized).replace(tzinfo=dt.timezone.utc)


def validate_logs(entry: Dict[str, Any], project_root: pathlib.Path, require_logs: bool) -> None:
    for key in ("build_log", "test_log"):
        log_info = entry.get(key, {})
        rel_path = log_info.get("path")
        if not rel_path:
            if require_logs:
                fail(f"{key} path missing in local pipeline evidence")
            continue
        log_path = project_root / rel_path
        if not log_path.exists():
            if require_logs:
                fail(f"{key} file '{rel_path}' not found. Re-run make pipeline.local.")
            continue
        recorded_hash = log_info.get("sha256")
        if recorded_hash:
            actual_hash = sha256(log_path)
            if actual_hash != recorded_hash:
                fail(
                    f"{key} hash mismatch ({actual_hash} != {recorded_hash}). Re-run make pipeline.local." 
                )


def main() -> None:
    args = parse_args()
    status_path = pathlib.Path(args.status_file).resolve()
        status_exists = status_path.exists()
        if not status_exists:
            if args.skip_if_missing:
                print(f"⚠️  Local pipeline status file '{status_path}' is missing; skipping validation.")
                return
            else:
                fail(f"Local pipeline status file '{status_path}' is missing")

    data = json.loads(status_path.read_text())
    status = (data.get("status") or "").lower()

    if status != "success":
        if args.allow_empty and status in {"", "uninitialized"}:
            print("⚠️  Local pipeline evidence is uninitialized; skipping strict validation.")
            return
        fail("Local pipeline has not been recorded as success. Run 'make pipeline.local' first.")

    recorded_commit = data.get("commit")
    try:
        current_commit = (
            subprocess.check_output(["git", "rev-parse", "HEAD"], cwd=status_path.parent)
            .decode()
            .strip()
        )
    except subprocess.CalledProcessError as exc:
        fail(f"Unable to resolve current commit: {exc}")

    if recorded_commit != current_commit:
        fail(
            "Local pipeline evidence was generated for commit "
            f"{recorded_commit}, but current HEAD is {current_commit}."
        )

    timestamp_raw = data.get("timestamp")
    if not timestamp_raw:
        fail("Local pipeline evidence timestamp missing")

    if args.max_age_minutes > 0:
        try:
            evidence_time = as_datetime(timestamp_raw)
        except ValueError as exc:
            fail(f"Invalid timestamp format in local pipeline evidence: {exc}")
        now = dt.datetime.now(tz=dt.timezone.utc)
        age_minutes = (now - evidence_time).total_seconds() / 60
        if age_minutes > args.max_age_minutes:
            fail(
                f"Local pipeline evidence is {age_minutes:.1f} minutes old (>{args.max_age_minutes} min)."
            )

    project_root = status_path.parents[1]
    validate_logs(data, project_root, args.require_logs)
    print("✅ Local pipeline evidence verified.")


if __name__ == "__main__":
    main()
