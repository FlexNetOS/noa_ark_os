# Llama.cpp Server Configuration - MAXIMIZE GPU UTILIZATION (70%+ VRAM)
server:
  host: 127.0.0.1
  port: 8080
  threads: 64  # Maximized CPU threads
  gpu_layers: 99  # ALL layers on GPU
  gpu_split: "32000,32000"  # Full utilization of BOTH RTX 5090s
  main_gpu: 0
  tensor_split: "1,1"  # Equal distribution
  
  # MAXIMUM MEMORY USAGE
  gpu_memory_fraction: 0.95  # Use 95% of available VRAM

models:
  - name: default
    path: ./models/
    context_size: 32768  # DOUBLED from 16K to 32K
    batch_size: 8192  # QUADRUPLED from 2048 to 8192
    n_parallel: 32  # INCREASED from 8 to 32 concurrent requests
    
    # CACHE IN VRAM (not system RAM)
    cache_type_k: f16
    cache_type_v: f16

inference:
  temperature: 0.7
  top_p: 0.9
  top_k: 40
  repeat_penalty: 1.1
  max_tokens: 8192  # DOUBLED from 4096
  
  # ALL OPTIMIZATIONS ENABLED
  flash_attention: true
  low_vram: false  # DISABLED - we have 64GB!
  use_mmap: false  # Load directly to VRAM
  use_mlock: true  # Lock in memory permanently
  no_kv_offload: false  # Keep KV cache on GPU
  
  # EXPERIMENTAL FEATURES ENABLED
  rope_scaling: true
  yarn_scaling: true

performance:
  timeout: 1800  # 30 minutes for large generations
  max_upload_size: 1GB  # 10x increase
  max_concurrent: 64  # DOUBLED again
  queue_size: 1000  # 10x increase
  
  # NO RATE LIMITING
  rate_limit: 0  # DISABLED
  
logging:
  level: info
  file: ./logs/server-unrestricted.log
  rotation: daily
  
monitoring:
  enable_metrics: true
  metrics_port: 8081
  report_gpu_usage: true
  report_memory: true
  
# TARGET: 70-85% GPU usage, 400-600 tok/s throughput
