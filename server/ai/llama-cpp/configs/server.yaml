# Llama.cpp Server Configuration
server:
  host: 127.0.0.1
  port: 8080
  threads: 8
  gpu_layers: 35  # Set to 0 for CPU-only

models:
  - name: default
    path: ./models/
    context_size: 8192
    batch_size: 512

inference:
  temperature: 0.7
  top_p: 0.9
  top_k: 40
  repeat_penalty: 1.1
  max_tokens: 2048

logging:
  level: info
  file: ./logs/server.log
  rotation: daily
